output_dir: 'outputs/t5-base'
per_device_train_batch_size: 64
per_device_eval_batch_size: 64
gradient_accumulation_steps: 1
learning_rate: 5e-5
weight_decay: 1e-4
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8
max_grad_norm: 1.0
num_train_epochs: 3
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_steps: 10
logging_dir: 'logs/t5-base'
save_strategy: epoch
seed: 42
bf16: false
fp16: true
fp16_opt_level: O1
ddp_find_unused_parameters: true
resume_from_checkpoint: null
gradient_checkpointing: false